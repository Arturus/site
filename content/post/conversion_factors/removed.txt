Заумное объяснение AUC из Википедии (интеграл ROC-кривой) можно пропустить,
смысл этой метрики на самом деле очень простой:

Выберем случайным образом
 два любых прогноза, которые выдала модель ($\hat{y}_1$ и $\hat{y}_2$),
и сопоставим их с правдой, т.е. соответствующими истинными значениями $y_1$ и $y_2$.
Для случая $y_1 > y_2$, оценим вероятность того, что и $\hat{y}_1 > \hat{y}_2$, и наоборот,
если $y_1 < y_2$, оценим $\Pr(\hat{y}_1 < \hat{y}_1)$. Значение этой вероятности
и будет метрикой AUC. Другими словами, AUC это вероятность, что в случайной паре прогнозов
бо́льшая величина прогноза будет соответствовать
большей истинной величине, т.е. ранжирование прогноза и истинного значения совпадёт.

Если модель не работает и выдаёт случайный прогноз, то вероятность того, что
в случайном наборе чисел одно будет больше другого, равна 50% - это "нулевой" уровень качества.
Если модель работает идеально, и даёт верный прогноз в 100% случаев,
то ранжирование всех пар будет совпадать, что даст AUC 100%. В реальной
жизни абсолютно точных прогнозов не бывает, и значения AUC будет находиться где то
между 50% и 100% -- чем больше, тем лучше.

AUC обычно используется в задачах классификации, но учитывая приведённое выше определение,
ничего не мешает расширить область применения и на регрессионные задачи,
т.е. предсказания непрерывных величин.
В регрессионных задачах метрика AUC известна под названием *concordance*,
или *concordance index* (*С-index*), т.к. определение через площадь под ROC-кривой для регрессии
не имеет смысла.
